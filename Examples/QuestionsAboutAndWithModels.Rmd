---
title: "Asking Question With and About a Model"
author: "Statistical Rethinking, Chapter 4"
date: "February, 2017"
output:
  html_document: 
    fig_height: 3
  pdf_document: default
params:
  original: no
  updated: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.show = "hold")
require(mosaic)
require(rethinking)

knitr::opts_template$set(
  original = list(eval = params$original, include = params$original),
  updated = list(eval = params$updated, include = params$updated),
  cache = TRUE)

trellis.par.set(theme = theme.mosaic())
theme_set(theme_minimal())
```

## Surveying the forest

The examples here come mostly from the second half of Chapter 4 of 
*Statistical Rethinking*.  Before we get started, here is a useful way of dividing 
up the work of Bayesian inference into 3 phases:

  1. Determine (describe) the model
  
    This is when you determine the likelihood and the priors you will use
    (and how the likelihood is connected to your data).
    
  2. Fit the model
  
    This is where you turn your description of the model into code that 
    tells R how to fit the model for you.  Here will will use the `map()`
    function, but there are other ways to fit Bayesian models, and each has
    its own syntax for describing the model to R.

  3. Use the model
  
    Once we have the model, we can do stuff with it.  In partciular, we can ask
    to sorts of questions:
    
    a. Questions with the model:
    What is the model's view of the world? 
    What do the parameter estimates mean?
    How precisely does the model think it knows these parameters?
    What predictions does the model make?
    How precisely does the model think it can make these predictions?
    
    b. Questions about the model: 
    Ditting the fitting algorithm work well?
    Is the model good? 
    Could it be improved?
    
    Often questions about the model are answered by comparing what the
    model "thinks" to data or some other informaiton to see how they match.

    Since a Bayesian model produces *distributions*, using the model
    generally requires working with these distributions.  For numerical
    methods, we typically do this by creating **posterior samples** which
    we can summarize numerically or display graphically.
    
Don't lose sight of the forest when we are investigating a particular tree.

## The Howell Data

```{r, chunk4.38a, opts.label = "updated"}
# load data 
library(rethinking)
data(Howell1)
HowellAdults <-
  Howell1 %>% filter(age >= 18)

xyplot(height ~ weight, data = Howell1, groups = (age >= 18), 
       alpha = 0.7, pch = c(16, 1))
```

## A simple linear model

```{r}
# fit linear model (height predicted from weight)
m4.3 <- 
  map(
    alist(
      height ~ dnorm(mu, sigma),
      mu <- a + b * weight,
      a ~ dnorm(156, 100),
      b ~ dnorm(0, 10),
      sigma ~ dunif(0, 50)
    ),
    data = HowellAdults)
```


```{r, chunk4.40}
precis(m4.3)
```


```{r, chunk4.41}
precis(m4.3, corr = TRUE)
```

### Centering weight

Centering a variable is done by subtracting a fixed value from each value of the variable.  The fixed value is 
often chosen to be one of 

  * the mean value
  * the median value
  * a "benchmark" value (or some "nice" value)

Centering can be useful to

  * make paramters more interpretable (especially the intercept).
  * improve the numerics of the fitting algorithm (by avoiding extremely large or small numerical values.
  
Here, let's center weight by substacting the mean:

```{r, chunk4.42a, opts.label = "updated"}
mean( ~ weight, data = HowellAdults)
HowellAdults <- 
  HowellAdults %>% 
  mutate(weight.c = weight - mean(weight))
```


And refit the model using the centered weight.

```{r, chunk4.43a, opts.label = "updated"}
m4.4 <- 
  map(
    alist(
      height ~ dnorm(mu, sigma),
      mu <- a + b * weight.c,
      a ~ dnorm(178, 100),
      b ~ dnorm(0, 10),
      sigma ~ dunif(0, 50)
    ),
    data = HowellAdults)
```

### Centered vs. Uncentered models

The two models `m4.3` and `m4.4` are essentially the same model, but the intercept values are different.  In the original model, 
the intercept gives "the mean height of people who weigh 0 kg".  In the new model, the intercept is "the mean height for people who
are average weight".  Besides being more interpretable, the new model also has the advantage that the two parameter estimates are 
no longer corrleated.  (In the original model, they were strongly negatively correlated.)


```{r, chunk4.44}
precis(m4.3, corr = TRUE)
precis(m4.4, corr = TRUE)
```

## Displaying the model(s)

One weakness of the `lattice` graphics system is that is a bit clunky to overlay multiple things on the same plot.  (Both base graphics and `ggplot2` graphics 
make this easier.  Here are three ways to overlay the fitted line on top of the data using `lattice`.

```{r, chunk4.45a, opts.label = "updated"}
# lattice with custom panel function to overlay multiple things
xyplot(height ~ weight, data = HowellAdults,
       panel = function(x, y, ...) {
         panel.abline(a = coef(m4.3)["a"], b = coef(m4.3)["b"])
         panel.xyplot(x, y, ...)
       }
)
```

This plot demonstrates that both models are the same -- just described differently.

```{r, chunk4.45b, opts.label = "updated", fig.keep = "last"}
# using plotFun() with add = TRUE to plot a function on top of points
xyplot(height ~ weight, data = HowellAdults)
plotFun(a + b * x ~ x, 
        a = coef(m4.3)["a"], b = coef(m4.3)["b"], 
        add = TRUE, col = "gray80", lwd = 4)
plotFun(a + b * (x - 45.05) ~ x, a = coef(m4.4)["a"], b = coef(m4.4)["b"], add = TRUE, col = "red")
```

```{r, chunk4.45c, opts.label = "updated"}
# creating a separate function for use with plotFun()
line.fit <- function(x, a = coef(m4.3)["a"], b = coef(m4.3)["b"]) {
  a + b * x
}
xyplot(height ~ weight, data = HowellAdults)
plotFun(line.fit(height) ~ height, add = TRUE, col = "red")
```

## Posterior sampling


### Using a Small Data Set

For comparison, let's also consider a model fit with much less data.

```{r, chunk4.48a, opts.label = "updated"}
HowellSmall <- HowellAdults %>% sample(20)      # 20 random adults
m4.3small <- 
  map(alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * weight,
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ),
  data = HowellSmall)
```


### Looking at the posterior parameter distributions

We can use posterior sampling to find out what a model thinks about the distributions of its parameters

```{r, chunk4.46a, opts.label = "updated"}
# good idea: name your posterior samples after the model you are sampling from
# 5000 posteriar samples of the model parameters (a, b, and sigma)
m4.3.post <- extract.samples(m4.3, n = 5000)
m4.3small.post <- extract.samples(m4.3small, n = 5000)
head(m4.3.post)  # the first few rows
```

As expected, the posterior distributions are much narrower when we use all the data.

```{r, chunk4.49a, opts.label = "updated"}
precis(m4.3small)
precis(m4.3)
densityplot( ~ b, data = m4.3small.post, 
             main = "posterior for slope (small data)",
             xlim = c(0, 2))
densityplot( ~ b, data = m4.3.post, 
             main = "posterior for slope (all data)",
             xlim = c(0, 2))

```

### Posterior lines

Since each row of our posterior sample data includes all the parameters, we con construct a line (or whatever other function we choose to fit)
for each row.  The plots below display the first 50 of these for both the small and large data sets.

```{r}
xyplot(height ~ weight, data = HowellSmall,
       xlim = c(30, 65),
       ylim = c(130, 180),
       col = rangi2,
       main = concat("N = ", nrow(HowellSmall)),
       panel = function(x, y, ...) {
         panel.xyplot(x, y, ...)
         # plot the lines, with transparency
         for (i in 1:50) {  # first 50 posterior samples
           panel.abline(
             a = m4.3small.post$a[i], 
             b = m4.3small.post$b[i], 
             col = "black", alpha = 0.1)
         }
       }
)

xyplot(height ~ weight, data = HowellAdults,
       xlim = c(30, 65),
       ylim = c(130, 180),
       col = rangi2,
       main = concat("N = ", nrow(HowellAdults)),
       panel = function(x, y, ...) {
         panel.xyplot(x, y, ...)
         # plot the lines, with transparency
         for (i in 1:50) {    # first 50 posterior samples
           panel.abline(
             a = m4.3.post$a[i], 
             b = m4.3.post$b[i], 
             col = "black", alpha = 0.1)
         }
       }
)
```


### How tall are people who weight 50 kg?

We can compue the model predicted value for the height of a person who weighs 50 kg
for each row of our posterior samples.  We'll add that infomration to our 
posterior samples data frame.

```{r, chunk4.50a, opts.label = "updated"}
m4.3small.post <-
  m4.3small.post %>%
  mutate(mu_at_50 = a + b * 50)
m4.3.post <-
  m4.3.post %>%
  mutate(mu_at_50 = a + b * 50)
```

```{r, chunk4.51a, opts.label = "updated"}
densityplot( ~ mu_at_50, data = m4.3small.post, 
             main = "Small Data",
             lwd = 2, xlim = c(150, 170),
             xlab = expression(paste(mu, " | weight=50")))
densityplot( ~ mu_at_50, data = m4.3.post,
             main = "All Data",
             lwd = 2, xlim = c(150, 170),
             xlab = expression(paste(mu, " | weight=50")))
```


```{r, chunk4.52a, opts.label = "updated"}
HPDI(m4.3small.post$mu_at_50, prob = 0.89)
HPDI(m4.3.post$mu_at_50, prob = 0.89)
```

### The link() function

The link() funciton automates this process for us, and makes it easy to do it at many values of `weight` all at once.
By default, it does this 1000 times for each observation in the data set.  (But we can tell it how many times
we want and we can give it specific values to evaluate at.)

```{r, chunk4.53}
mu <- link(m4.3)
mu.small <- link(m4.3small)
str(mu)
str(mu.small)
```

```{r}
mu40 <- link(m4.3, data = data.frame(weight = 40))
densityplot( ~ mu40[, 1], data = m4.3.post,
             main = "All Data",
             xlim = c(145, 162),
             lwd = 2, xlab = expression(paste(mu, " | weight=40")))
mu50 <- link(m4.3, data = data.frame(weight = 50))
densityplot( ~ mu50[, 1], data = m4.3.post,
             main = "All Data",
             xlim = c(145, 162),
             lwd = 2, xlab = expression(paste(mu, " | weight=50")))
```



```{r, chunk4.54a, opts.label = "updated"}
# define sequence of weights to compute predictions for
# these values will be on the horizontal axis
m4.3.pred <-
  data.frame(weight = seq(from = 25, to = 70, by = 1))

# use link to compute mu for each sample from posterior (rows)
# and for each weight in m4.3.pred (columns)
m4.3.mu <- 
  link(
    m4.3, 
    data = m4.3.pred,
  )
str(mu)
```



```{r, chunk4.55a, fig.keep = "last"}
# The shape of mu coming out of link() is backwards for lattice
# t() flips rows and columns to make it work
xyplot(t(m4.3.mu) ~ weight, data = m4.3.pred, alpha = 0.1, col = rangi2, pch = 16)
```



```{r, chunk4.56}
# summarize the distribution of mu, storing resutls in m4.3.pred
m4.3.pred <-
  m4.3.pred %>% 
  mutate (
    mu.mean = apply(m4.3.mu, 2, mean), 
    mu.HPDI.lo = apply(m4.3.mu, 2, HPDI, prob = 0.89)[1, ], 
    mu.HPDI.hi = apply(m4.3.mu, 2, HPDI, prob = 0.89)[2, ] 
  )

head(m4.3.pred)
```



```{r, chunk4.57a, fig.keep = "last"}

# plot the MAP line, aka the mean mu for each weight
xyplot(mu.HPDI.hi + mu.mean + mu.HPDI.lo ~ weight, data = m4.3.pred, type = "l")
# plot raw data, fading out points to make line and interval more visible
plotPoints(height ~ weight, data = HowellAdults, col = rangi2, alpha = 0.3, add = TRUE)
```


### Simulated heights

Instead of looking at the mean height for each weight, we could look
at simulated heights.  This takes into account the variability
from person to person as well as the uncertainty
in the paramter estimates.
Once again `sim()` produces a matrix with a row for 
each posterior sample and a column for each weight we are considering.


```{r, chunk4.59a, opts.label = "updated"}
# advice: choose a small value of n first to make sure things are working
#         then come back and make it larger for the final version
sim.height <- sim(m4.3, data = m4.3.pred, n = 1e4)  
str(sim.height)
```

We can add information based on thise simulated heights to our `m4.3pred` object.


```{r, chunk4.60a, opts.lable = "updated"}
m4.3.pred <- 
  m4.3.pred %>% 
  mutate(
    height.PI.lo =  apply(sim.height, 2, PI, prob = 0.89)[1, ],
    height.PI.hi =  apply(sim.height, 2, PI, prob = 0.89)[2, ]
    )
```


```{r, chunk4.61a, fig.keep = "last"}
# Plot intervals for mu and for simulated height
xyplot(height.PI.hi + mu.HPDI.hi + mu.mean + mu.HPDI.lo + height.PI.lo ~ weight, 
       data = m4.3.pred,
       type = "l",
       ylab = "height") 

# add raw data
plotPoints(height ~ weight, HowellAdults, col = rangi2, alpha = 0.5, add = TRUE)
```



### How link() and sim() work

This is roughly how `link()` works:

```{r}
post <- extract.samples(m4.3)
weight.seq <- 25:70

# compute the fit 
#  * for each weight in our sequence (columns), and 
#  * for each set of parameter estimates in our posterior sample (rows)

mu <- 
  sapply(
    weight.seq, 
    function(weight) {
      post$a + post$b * weight
    }
  )
str(mu)
```

This is roughly how `sim()` works.

```{r}
post <- extract.samples(m4.3)
weight.seq <- 25:70

# simulate an individual height 
#  * for each weight in our sequence (columns), and 
#  * for each set of parameter estimates in our posterior sample (rows)

sim.height <- sapply(weight.seq, function(weight)
  rnorm(
    n = nrow(post),
    mean = post$a + post$b * weight,  # average height at given weight
    sd = post$sigma                 # add person-to-person variability
  ))
str(sim.height)
```


## A quadratic model using standardized weights

Now lets fit a model that uses all the data (kids as well as adults). 


Standardized weights are like centered weights, but then we also divide
by the standard deviation.  So most values will be between -3 and 3.  This 
is a "unitless" version of weight that measures how many standard deviations
above or below the mean a given value is.
`zscore()` automates the calculations.

```{r, chunk4.65a, opts.label = "updated"}
require(mosaic)  # for zscore()
Howell1 <-
  Howell1 %>%
  mutate(
    weight.s = zscore(weight),
    weight.s2 = weight.s^2
  )
```
A line fits badly.

```{r, fig.keep = "last"}
m4.5a <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * weight.s,
    a ~ dnorm(178, 100),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ),
  data = Howell1
)
precis(m4.5a)
coef(m4.5a)
xyplot(t(link(m4.5a, n = 100, 
              data = data.frame(weight.s = seq(-2.5, 2.5, by = 0.1))
              )) ~ weight.s, 
              data = data.frame(weight.s = seq(-2.5, 2.5, by = 0.1)),
           alpha = 0.3, col = "green")
plotPoints(height ~ weight.s, data = Howell1, add = TRUE, alpha = 0.5)
ladd(panel.abline(a = 138.26,  b = 25.93))
```

Now let's fit a quadratic model.  Using standardized weights avoids having 
very large values of `weight^2`.

```{r, chunk4.66a, opts.label = "updated"}
m4.5 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1 * weight.s + b2 * weight.s2,
    a ~ dnorm(178, 100),
    b1 ~ dnorm(0, 10),
    b2 ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ),
  data = Howell1
)
```


```{r, chunk4.67}
precis(m4.5)
```

```{r, chunk4.68a, opts.label = "updated"}
m4.5.pred <-
  data_frame(
    weight.s = seq(from = -2.2, to = 2, length.out = 30),
    weight.s2 = weight.s^2
  )
mu <- link(m4.5, data = m4.5.pred)
sim.height <- sim(m4.5, data = m4.5.pred)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
m4.5.pred <-
  m4.5.pred %>%
  mutate(
    mu.mean = apply(mu, 2, mean),
    mu.lo = apply(mu, 2, PI)[1,],
    mu.hi = apply(mu, 2, PI)[2,],
    sim.lo = apply(sim.height, 2, PI)[1,],
    sim.hi = apply(sim.height, 2, PI)[2,]
    )
```


```{r, chunk4.69a, opts.label = "updated"}
xyplot(sim.hi + mu.hi + mu.mean + mu.lo + sim.lo ~ weight.s, 
       data = m4.5.pred, type = "l", 
       ylab = "height",
       auto.key = list(lines = TRUE, points = FALSE))
plotPoints(height ~ weight.s, data = Howell1, col = rangi2, alpha = 0.5, add = TRUE)
```
```{r, chunk4.69b, opts.label = "updated"}
ggplot(aes(x = weight.s), data = m4.5.pred) +
  geom_point(aes(y = height), data = Howell1, color = rangi2) +
  geom_line(aes(y = mu.lo), color = "navy") +
  geom_line(aes(y = mu.hi), color = "navy") +
  geom_line(aes(y = sim.lo), color = "red") +
  geom_line(aes(y = sim.hi), color = "red") 
```




```{r, chunk4.70a, opts.label = "updated"}
Howell1 <- 
  Howell1 %>% mutate(weight.s3 = weight.s^3)

m4.6 <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b1 * weight.s + b2 * weight.s2 + b3 * weight.s3,
    a ~ dnorm(178, 100),
    b1 ~ dnorm(0, 10),
    b2 ~ dnorm(0, 10),
    b3 ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)
  ),
  data = Howell1
)
```

```{r}
m4.6.pred <-
  data_frame(
    weight.s = seq(from = -2.2, to = 2, length.out = 30),
    weight = mean(~ weight, data = Howell1) + weight.s * sd(~ weight, data = Howell1),
    weight.s2 = weight.s^2,
    weight.s3 = weight.s^3
  )
mu <- link(m4.6, data = m4.6.pred)
sim.height <- sim(m4.6, data = m4.6.pred)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
m4.6.pred <-
  m4.6.pred %>%
  mutate(
    mu.mean = apply(mu, 2, mean),
    mu.lo = apply(mu, 2, PI)[1,],
    mu.hi = apply(mu, 2, PI)[2,],
    sim.lo = apply(sim.height, 2, PI)[1,],
    sim.hi = apply(sim.height, 2, PI)[2,]
    )
```


```{r}
xyplot(sim.hi + mu.hi + mu.mean + mu.lo + sim.lo ~ weight.s, 
       data = m4.6.pred,  
       ylab = "height", type = "l", 
       auto.key = list(lines = TRUE, points = FALSE))
plotPoints(height ~ weight.s, data = Howell1, col = rangi2, alpha = 0.5, add = TRUE)
```

```{r}
xyplot(sim.hi + mu.hi + mu.mean + mu.lo + sim.lo ~ weight, 
       data = m4.6.pred,  
       ylab = "height", type = "l", 
       auto.key = list(lines = TRUE, points = FALSE))
plotPoints(height ~ weight, data = Howell1, col = rangi2, alpha = 0.5, add = TRUE)
```



