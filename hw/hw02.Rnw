
\documentclass{article}
\usepackage[margin=.5in,top=.3in,bottom=.4in,includehead,includefoot]{geometry}

\usepackage{problems}
\usepackage{probstat}


%\parindent=0pt
\parskip=3pt


%\usepackage{mathpple}
%\usepackage{fullpage}
\usepackage{alltt}
\usepackage{sfsect}
\usepackage{language}
\usepackage{longtable}

\usepackage[normalem]{ulem}

\newif\ifanswers
\answerstrue
\answersfalse

\def\blank#1{%
\ifanswers
\underline{{\large \sc #1}}
\else
\underline{\phantom{\Large \ \ \ \sc #1 \ \ \ }}
\fi
}

\def\dd#1{\medskip\noindent\textbf{\large #1}}
\def\be{\begin{enumerate}}
\def\ee{\end{enumerate}}
\def\bi{\begin{itemize}}
\def\ei{\end{itemize}}

\def\hwn#1{\relax}
\def\hwn#1{\ {\tiny (#1)}}
\def\hwn#1{$_{\mbox{\ \tiny #1}}$}
\def\hwn#1{~{\tiny #1}}

\usepackage[normalem]{ulem}
\usepackage{ifthen}
\usepackage{fancyhdr}
\usepackage[colorlinks, urlcolor = blue]{hyperref}
\include{myabbrev}


\renewcommand{\labelenumi}{\textbf{\arabic{enumi}}}
\newcommand{\myunderline}[1]{\uline{\phantom{Ay}#1\phantom{Ay}}}

\newcommand{\helper}[2][\relax]{(#2)\hwn{#1}}
\newcommand{\extra}[2][\relax]{[#2]\hwn{#1}}
\newcommand{\opt}[2][\relax]{\{#2\}\hwn{#1}}
\newcommand{\mand}[2][\relax]{{\bf \large #2}\hwn{#1}}
\renewcommand{\arraystretch}{1.4}

\begin{document}

<<setup, include = FALSE>>=
require(rstan)
require(rethinking)
require(mosaic)
require(rvest)
require(readr)
require(statisticalModeling)

knitr::opts_chunk$set(
  # background = c(1, 1, 1),
  size = "small",
  fig.height = 2.5,
  fig.width = 7,
  cache = TRUE
)
theme_set(theme_bw())
@

\pagestyle{fancy}

\chead{}
\lhead{Stat 341 -- Spring 2017}
\rhead{\thepage/\pageref{end}}
\lfoot{}
\cfoot{Created \today\ --- See web site for most current version.}
\rfoot{}

\begin{center}
\large
\subsection*{Problem Sets Between Tests 1 and 2}

\end{center}


\textit{Only turn in problems that are \textbf{not} bracketed.}
Bracketed problems are additional problems you can look at.
Round brackets indicate problems that may help you with problems that
are assigned; square brackets are additional problems on material
that you should know, but you are not required to write up solutions;
curly brackets are truly optional and may contain extra nuggets that you
will not be required to know but may be interested in.

Additional assignments will be filled in over time.

\bigskip

\begin{center}

\iftrue
\begin{longtable}{|c|l|}
  \hline
  notation & meaning \\
  \hline
  unbracketed & assigned problem -- turn these in for grading \\
  $()$ & helper/warm-up problem  \\
  $[]$ & additional problems (you are responsible for content, but don't turn them in) \\
  \{\} & covers optional material \\
  \hline
\end{longtable}
\fi

\bigskip

\begin{longtable}{|c|c|c|p{4.3in}|}
  \hline
  PS & Due & Source & Problems \\
  \hline\hline
  \endhead
  9 & Wed 3/8 & Rethinking 6 &
  \mand[entropy]{6E3--6E4}
  \\
  & & Additional Problems &
  \mand[entropy]{1}
  \mand[entropy]{2}
\\ \hline
  10 & Fri 3/17 & Rethinking 6 &
 \mand[same data]{6M3}
 \mand[narrow prior]{6M4}
 \mand[compare models]{6H1}

 \mand[train vs. test]{6H4}
 \mand[train vs. test]{6H5}

 \medskip

 \emph{Use the replacement code below in place of the R code 6.31 and 6.32.}

\\ \hline
  11 & Wed 3/29 & Rethinking 6 &
  \mand[selection v averaging]{6M2}
  \mand[over/under fitting]{6M5--6M6}
  \mand[plotting models]{6H2}
  \mand[model averaging]{6H3}
\\ \hline
12 & Wed 4/5 &
Rethinking 7 &
\mand[loo]{7H3}
\\
 & & Rethinking 8 &
\mand{8H3 -- 8H4}
\\
& & & Note: You can find replacement code for the code the author
provides in the usual place online.
\\ \hline
13 & Thu 4/13 &
Rethinking 10 &
\mand[warm-up]{10E1 -- 10E2}
\mand[likelihood]{10M1}
\mand[link]{10M3}

\mand[map vs stan]{10H1}
\mand[chimp models]{10H2}
\mand[eagle pirates]{10H3}
\\ \hline
14 & Fri 4/21 &
Rethinking 12 &
\mand[shrinkage]{12E1}
\mand[multi-level]{12E2}
\mand[multi-level]{12E3}
\\ \hline
15 & Mon 4/24 &
Rethinking 12 &
\mand[tadpoles]{12M1}
\mand[tadpoles]{12M2}
\mand[priors]{12M3}
\mand[contraception]{12H1}
\\
&&&
Note: For problem 12M3, rather than have you all print out the model fitting and
\function{precis()} code, I've included it below.  You don't need to include it in
your print out.
\\
&&&
Note: \code{coeftab()} is useful for comparing coefficients across
different models.  I've updated the \pkg{rethinking} package so you
can use square brackets to grab just some rows/columns to save some paper.
Also, be sure to include \code{require(rstan)} as well as \code{require(rethinking)}
now.
\\ \hline
\end{longtable}
\end{center}

% \mand[regularizing prior]{6H6}


\subsection*{Replacement Code}

\subsection*{12M3}

This isn't really replacement code, I'm just going to save you some time by giving you
code and output that you don't need to repeat in the work you turn in.  You
can run the code and do other things if you like.  But don't include the code that
creates this model or the precis output in your homework.  Let's save a tree.

<<include = FALSE>>=
data(reedfrogs)
Frogs <- reedfrogs %>% mutate(tank = 1:n())   # make the tank cluster variable
m.12h2 <- map2stan(
  alist(
    surv ~ dbinom(density, p),
    logit(p) <- a_tank[tank],
    a_tank[tank] ~ dnorm(a, sigma),
    a ~ dnorm(0, 1),
    sigma ~ dcauchy(0, 1)
  ),
  data = Frogs, refresh = 0, iter = 4000
)
@

<<>>=
data(reedfrogs)
Frogs <- reedfrogs %>% mutate(tank = 1:n())   # make the tank cluster variable
m.12h3 <- map2stan(
  alist(
    surv ~ dbinom(density, p),
    logit(p) <- a_tank[tank],
    a_tank[tank] ~ dcauchy(a, sigma),
    a ~ dnorm(0, 1),
    sigma ~ dcauchy(0, 1)
  ),
  data = Frogs, refresh = 0, iter = 4000
)
@

<<fig.height = 6>>=
precis(m.12h3)  # only the highest level parameters
precis(m.12h3, depth = 2)      # all the parameters
plot(precis(m.12h3, depth = 2))
@

<<>>=
# this is mainly just to demo the new coeftab behavior
coeftab(m.12h2, m.12h3)[c("a", "sigma"), ]
@


\subsubsection*{R Code 6.31}
<<chunk6.31>>=
library(rethinking)
data(Howell1)
Howell <-
  Howell1 %>%  mutate(age.s = zscore(age))
set.seed(1000)     # so we all get the same "random" data sets
train <- sample(1:nrow(Howell), size = nrow(Howell) / 2)  # half of the rows
Howell.train <- Howell[ train, ]   # put half in training set
Howell.test  <- Howell[-train, ]   # the other half in test set
@

<<include = FALSE>>=
m <- map(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * age.s,
    a ~ dnorm(138, 10),
    b ~ dnorm(0, 10),
    sigma ~ dunif(0, 50)),
  data = Howell.train,
  start = list(a = 130, b = 4, sigma = 40)
)
-2 * logLik(m)
mu <- link(m, data = Howell.test) %>% apply(2, mean)
-2 * sum(dnorm(Howell.test$height, mu, coef(m)["sigma"], log = TRUE))
@

\subsubsection*{R Code 6.32}
<<chunk6.32, eval = FALSE>>=
# You need to come up with mu and sigma
sum(dnorm(Howell.test$height, mu, sigma, log = TRUE))
@


\subsection*{Additional Problems}

\begin{problem}
Making entropy larger.
\begin{enumerate}
\item
Which is larger:  $H(0.1, 0.3, 0.6)$ or $H(0.2, 0.2, 0.6)$?
\item
Let $\vec p = \tuple{p_1, p_2, p_3}$ and let
$\vec q = \tuple{p, p, p_3}$ where $p = \frac{p_1 + p_2}{2}$.
Compute $H(\vec p)$ and $H(\vec q)$.  Which is larger?
\item
Suppose a random process has $n$ outcomes.  Show that with
one exception, there is always another random process that
also has $n$ outcomes, but has higher entropy?
What is the one exception?  (The exception is the
random process with the maximal entropy among processes with $n$ outcomes.)
\end{enumerate}
\end{problem}

\begin{solution}
<<>>=
H <- function(p) - sum(p[p>0] * log(p[p>0]))
H(c(0.1, 0.3, 0.6))
H(c(0.2, 0.2, 0.6))
@

Let $h(p) = p \log(p) + (s - p) \log(s-p)$ where $s$ is fixed.  I've used little
h because this is a little part of the full entropy where the probabilities of
two events sum to $s$.  I've left off the negative sign to simplify the derivative
below.


$h'(p) = \log(p) + p \frac{1}{p} - \log(s-p) - (s - p) \frac{1}{s-p} =  \log(p) - \log(s-p)$.
So $h'(p) = 0$ when $p = s-p$.
This means that the largest entropy happens when these two events have the same
probability.

We play this game any time thre are two unequal probabilities, so the the maximal
entropy is acheived when all the probabilities are equal.

(This can also be deonstrated by clever algebra and log rules,
but I find this more instructive and less messy.)
\end{solution}

\begin{problem}
Compute the entropy of tossing two coins two different ways:
\begin{enumerate}
\item Consider the outcomes to be 0, 1 or 2 heads.
\item Consider the outcomes to be HH, HT, TH, or TT.
\end{enumerate}
How do the results compare?  Can you generalize?
\end{problem}


\label{end}
\end{document}

